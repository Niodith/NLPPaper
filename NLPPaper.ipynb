{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Some paragraph](#paragraph1)\n",
    "    1. [Sub paragraph](#subparagraph1)\n",
    "3. [Another paragraph](#paragraph2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "In the area of expertise of handling complex data machines still outmatch (most) humans. In the area of Data Science processing of named data machine learning became a key component in the repertoire of most data scientists.\n",
    "But humans outmatch machines in one important part - dealing and handling natural languges - this subdiscipline of data scienes needed most of the time a human to yield desired results.\n",
    "\n",
    "The fields of *natural language processing* and *natural language generation* are key to use the benefits of machine learning to its fullest - while keeping the ability of machines to deal with data(structs) way more efficient than a human it adds a \"human\" coponent to the machine to get natrual seeming resuts.\n",
    "\n",
    "*Natural language processing* dates back before even the digital computer were invented - a famous example is Warren Weaver whose efforts of code decoding were mainly during World War II. [https://www.cs.bham.ac.uk/~pjh/sem1a5/pt1/pt1_history.html]\n",
    "Natural language processing enables the machine to get its data it needs for in depth analysis out of seemingly unstructured languages and the possibility to learn from *human* generated text to improve itself for more natural *machine* generated text.\n",
    "\n",
    "*Natural language generation* is compared a newer technology - first mostly used in chatbots it found its way into the toolkits of data scientist and lingustics alike. Since 2014 until 2017 job possibilities in natural language generation and deep learning grew from a not mention worthy number to 41,000 jobs - a significant increase which is likely to be linked to an higher demand in this diciplines. [https://www.gartner.com/smarterwithgartner/nueral-networks-and-modern-bi-platforms-will-evolve-data-and-analytics/]\n",
    "\n",
    "\n",
    "An important use of *natural language generation* is the so called *data-to-text* generation- it is used to mimicry human wiriting and text based on the data a human normally would refer to in his or her text. The most used example for this is RotoWire a webpage which contains the statistics of match outcomes of many different sports plus the journalistic description of named matches e.G. NBA basketball matches. [https://www.rotowire.com/]\n",
    "\n",
    "Data-to-text generation models have to things that need to be considered carefully: \n",
    "\n",
    "\n",
    "This matching of data-to-text made buy a human is a welcome base for machine learning to teach that task to a machine. With a *encoder - decoder* system the RotoWire datasets can be \"translated\" into seemingly human written texts.[https://aaai.org/ojs/index.php/AAAI/article/view/4653]\n",
    "An *encoder* acts as an combinational circuit in which a dataset will be encoded into the desired form with the corresponding algorithm while an *decoder* reverts this process with the encoded data.\n",
    "\n",
    "Designing data-to-text models gives rise to two main challenges:\n",
    "1) understanding structured data and 2) generating associated descriptions. \n",
    "\n",
    "Referring to the work of ClÂ´ement Rebuffel et al. in their work *A Hierarchical Model for Data-to-Text\n",
    "Generation* [https://arxiv.org/pdf/1912.10011v1.pdf] we will use the same approach to improve the most used which are model by encoder-decoder architecture [source 2 in paper http://arxiv.org/abs/1409.0473], where the data structure is firstly encoded sequentially by an encoder which has a fix-sized vectorial represantation.\n",
    "For improvement we will use an more hierachial approach, which focueses on the data-structure encoding, but with an classical decoder approch as it is found often in preious works. [https://www.aclweb.org/anthology/D17-1239.pdf]\n",
    "\n",
    "Overall following adaptions and improvements will be made: \n",
    "\n",
    "- The basic structure will be following a two-level architecture design by first encoding all the entities based on their elements and after that encoding the data structs on the same basis\n",
    "- A Transformer encoder will be introduced to the used data-to-text model to verify a stable encoding of each entity in comparison unaffected by their original order\n",
    "- An hierarchical attention mechanism for computing the hierachical context will be added dor feeding the decoder \n",
    "\n",
    "\n",
    "The experimental setup will be using data structs from the RotoWire benchmark which contains around 5000 tables with stats of basketball games with matching human made descriptions. This model will be compared to several state of the art approches and will be performing more efficient and get an higher BLUE score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
